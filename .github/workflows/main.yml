name: Deploy Databricks Notebooks

on:
  push:
    branches:
      - main  # Adjust the branch name as needed

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'  # Ensure Python 3.x is installed
    
      
      - name: Install Databricks CLI
        uses: databricks/setup-cli@main
        
      - name: Configure Databricks CLI
        run: |
           databricks configure --token "${{ secrets.DATABRICKS_TOKEN }}" --host "${{ secrets.DATABRICKS_HOST }}"
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      - name: Clone GitHub Repository
        run: |
          git clone https://github.com/mayo8624/GCP-DEMO.git
          cd GCP-DEMO
        # Navigate to the cloned repository directory

      - name: Import Notebooks to Databricks Workspace
        env:
           DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
           DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
           LOCAL_FOLDER_PATH: demodemo
           DATABRICKS_FOLDER_PATH: /live
        run: |
          echo "Importing notebooks from $LOCAL_FOLDER_PATH to Databricks workspace folder: $DATABRICKS_FOLDER_PATH"
          find "$LOCAL_FOLDER_PATH" -name '*.py' -type f | while read notebook_file; do
          notebook_name=$(basename "$notebook_file")
          echo "Importing $notebook_name..."
          databricks workspace import \
            --language PYTHON \
            --format SOURCE \
            --file "$notebook_file" \
            "$DATABRICKS_FOLDER_PATH/$notebook_name"
          done  
      # - name: import Demo
      #   run: |
      #     target_folder="/live"
      #     # Define the target Databricks folder
      #     # Iterate through all Python notebooks in the repository and import them one by one
      #     find . -name '*.py' | while IFS= read -r notebook_file; do
      #      notebook_name=$(basename "$notebook_file")
      #      target_path="$target_folder/$notebook_name"
  
      #     # Check if the notebook already exists in the target folder
      #      if databricks workspace ls "$target_path" &> /dev/null; then
      #       echo "Notebook $notebook_name already exists in Databricks folder: $target_folder"
      #     # Handle the conflict (e.g., skip, overwrite, rename)
      #     # Example: Handle the conflict by skipping the import
      #       echo "Skipping import of $notebook_name"
      #      else
      #       echo "Importing $notebook_name to Databricks folder: $target_folder"
      #       # Perform the import
      #       databricks workspace import --language PYTHON --format SOURCE --file "$notebook_file" "$target_path"
      #      fi
      #     done

      # - name: import Notebooks Demo
      #   run: | 
      #    echo "Uploading notebooks from $LOCAL_NOTEBOOKS_PATH to $REMOTE_NOTEBOOK_PATH in the workspace $DATABRICKS_HOST"
      #    databricks workspace import_dir --overwrite "${LOCAL_NOTEBOOKS_PATH}" "${REMOTE_NOTEBOOK_PATH}" --debug
      #   shell: bash
      #   env:
      #      LOCAL_NOTEBOOKS_PATH: 'demodemo'
      #      REMOTE_NOTEBOOK_PATH: 'live'
      
      # - name: Import Notebooks to Databricks
      #   run: |
      #     target_folder="/live"
      #     # Define the target Databricks folder
      #     # Find all Python notebooks in the repository and import them one by one
      #     find . -name '*.py' | while IFS= read -r notebook_file; do
      #       notebook_name=$(basename "$notebook_file")
      #       echo "Importing $notebook_name to Databricks folder: $target_folder"
      #       databricks workspace import --language PYTHON --format SOURCE --file "$notebook_file" "$target_folder/$notebook_name"
      #     done
      #   shell: bash


      # - name: Copy Folder to Databricks Workspace
      #   run: |
      #     # Define local folder path to be copied
      #     local_folder_path="demodemo"

      #     # Define target folder path in Databricks workspace where the folder will be copied
      #     databricks_folder_path="/live"

      #     echo "Copying folder from $local_folder_path to Databricks workspace folder: $databricks_folder_path"
          
      #      databricks workspace mkdirs "$databricks_folder_path"  # Create the target folder if it doesn't exist
      #      rsync -av --ignore-existing "$local_folder_path"/* "$databricks_folder_path/"
      #     # # Use Databricks CLI to recursively copy the local folder to Databricks workspace
      #     # databricks workspace import_dir --overwrite "$local_folder_path" "$databricks_folder_path"

      # - name: Import Notebooks to Databricks Working
      #   run: |
      #     # Define local path to the folder containing notebooks
      #     local_folder_path="GCP-DEMO"

      #     # Define target path in Databricks workspace where notebooks will be imported
      #     databricks_folder_path="/live"

      #     echo "Importing notebooks from $local_folder_path to Databricks workspace folder: $databricks_folder_path"

      #     # Iterate through notebooks in the local folder and import them to Databricks workspace
      #     find "$local_folder_path" -name '*.py' -type f | while read notebook_file; do
      #       notebook_name=$(basename "$notebook_file")
      #       echo "Importing $notebook_name..."
      #       databricks workspace import --language PYTHON --format SOURCE --file "$notebook_file" "$databricks_folder_path/$notebook_name"
      #     done

     


      
      # - name: Import Notebooks to Databricks
      #   run: |
      #     target_folder="/live"
      #     find . -name '*.py' | while read notebook_file; do
      #       notebook_name=$(basename "$notebook_file")
      #       destination_file="$target_folder/$notebook_name"
      #       if ! test -f "$destination_file"; then
      #        echo "Importing $notebook_name to Databricks folder: $target_folder"
      #        databricks workspace import --language PYTHON --format SOURCE --file "$notebook_file" "$destination_file"
      #       else
      #        echo "Skipping existing notebook: $destination_file"
      #       fi
      #     done

      
          # Define the target Databricks folder
          # Iterate through all Python notebooks in the repository and import them
          # find . -name '*.py' | while read notebook_file; do
          #   notebook_name=$(basename "$notebook_file")
          #   echo "Importing $notebook_name to Databricks folder: $target_folder"
          #   databricks workspace import --language PYTHON --format SOURCE --file "$notebook_file" "$target_folder/$notebook_name"
          # done
          

          
      # - name: Import gold.py to Databricks
      #   run: |
      #       databricks workspace import --language PYTHON --format SOURCE --file Gold.py /live/gold
      #      # databricks workspace import --language PYTHON /gold2.py
      #     # databricks workspace import --language PYTHON gold.py /workspace/live
      # - name: Import notebooks to Databricks
      #   run: |
      #     # Iterate through notebooks in the repository
      #      for notebook_file in $(find . -name '*.py'); do
      #        databricks workspace import --format SOURCE --language PYTHON --overwrite --file "$notebook_file" "${{ env.DATABRICKS_WORKSPACE_PATH }}"
      #      done
        
        # env:
        #   DATABRICKS_WORKSPACE_PATH: /Users/prathamesh.bhamare@celebaltech.com/abcd2  # Replace with your desired Databricks workspace path
